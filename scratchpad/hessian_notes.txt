----------------

Finding hessian using nothing but values of x'*H*x for many random x.

minimize_H  sum_x((x'*H*x - x'*A*x)^2)

grad = 2*sum_x((x'*H*x - x'*A*x)*(x'*x))

sum_x((x'*H*x - x'*A*x)*(x'*x)) = 0
= sum_x((x'*H*x)*(x'*x) - (x'*A*x)*(x'*x)) = 0

sum_x((x'*H*x)*(x'*x)) = sum((x'*A*x)*(x'*x))
????

Try to optimize the step size:
0 = grad_a sum_x((x'*(H+aG)*x - x'*A*x)^2)
  = sum_x(2*(x'*(H+aG)*x - x'*A*x)*x'*G*x)
  = sum_x((x'*(H+aG)*x)*x'*G*x - (x'*A*x)*(x'*G*x))
  = sum_x((x'*H*x + x'*aG*x)*x'*G*x - (x'*A*x)*(x'*G*x))
  = sum_x((x'*H*x)*(x'*G*x) + a*(x'*G*x)^2 - (x'*A*x)*(x'*G*x))
  = sum_x((x'*H*x)*(x'*G*x) + a*(x'*G*x)^2 - (x'*A*x)*(x'*G*x))

-----------------

Finding hessian using second-order info.

We have a batch of inputs x and outputs y from H, i.e. Hx = y.

Want to find matrix H to minimize sum_i((H*x_i - y_i)^2).
Constrain H to be symmetric, i.e. H = A'*A:
loss = sum_i(((A' + A)*x_i - y_i)^2)
let r_i = ((A' + A)*x_i - y_i)
grad(a_ij) = sum_n(r_ni * x_nj + r_nj * x_ni)
grad(A) = sum_i(r*x_i' + x_i'*r)

Letting X and Y be big matrices where columns are xs and ys.

grad(A) = (AX + Y)*X' + X*(AX + Y)'

Now let's figure out the optimal step size:

grad = G, and G = G' by definition in the above formula
let H = our solution matrix.

loss = ||(H+a*G)*X - Y||^2 = ||H*X + a*G*X - Y||^2
optimal a = dot(Y - H*X, G*X) / dot(G*X, G*X)
